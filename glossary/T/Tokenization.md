# Tokenization
Tokenization is the process of substituting sensitive information with a token with no inherit value to a potential attacker.
## Tags
Source_URL:https://en.wikipedia.org/wiki/Tokenization_(data_security)
